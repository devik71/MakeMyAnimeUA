#!/usr/bin/env python3
"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ¸ GPU Ñ‚Ð° CUDA Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÐ¸ Ð´Ð»Ñ Whisper Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð°Ñ†Ñ–Ñ—
"""

import torch
import sys

def check_gpu():
    print("ðŸ” Checking GPU and CUDA support...")
    print("=" * 50)
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ PyTorch Ð²ÐµÑ€ÑÑ–ÑŽ
    print(f"ðŸ“¦ PyTorch version: {torch.__version__}")
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ CUDA Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ–ÑÑ‚ÑŒ
    cuda_available = torch.cuda.is_available()
    print(f"âš¡ CUDA available: {cuda_available}")
    
    if cuda_available:
        # Ð†Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ Ð¿Ñ€Ð¾ GPU
        gpu_count = torch.cuda.device_count()
        print(f"ðŸŽ® Number of GPUs: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"ðŸŽ® GPU {i}: {gpu_name}")
            print(f"ðŸ’¾ GPU {i} Memory: {gpu_memory:.1f} GB")
        
        # ÐŸÐ¾Ñ‚Ð¾Ñ‡Ð½Ð¸Ð¹ GPU
        current_device = torch.cuda.current_device()
        print(f"ðŸŽ¯ Current GPU: {current_device}")
        
        # Ð¢ÐµÑÑ‚ GPU
        print("\nðŸ§ª Testing GPU with simple tensor operation...")
        try:
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.mm(x, y)
            print("âœ… GPU test successful!")
        except Exception as e:
            print(f"âŒ GPU test failed: {e}")
            return False
    else:
        print("âš ï¸  CUDA not available. You can still use CPU for transcription, but it will be slower.")
        print("ðŸ’¡ To enable GPU support, install PyTorch with CUDA:")
        print("   pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n" + "=" * 50)
    
    # Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ñ–Ñ— Ð´Ð»Ñ Whisper
    print("ðŸ“‹ Whisper Model Recommendations:")
    if cuda_available:
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ðŸ’¾ Your GPU has {gpu_memory:.1f} GB memory")
        
        if gpu_memory >= 8:
            print("âœ… Recommended: large model (best quality)")
        elif gpu_memory >= 4:
            print("âœ… Recommended: medium model (good quality)")
        elif gpu_memory >= 2:
            print("âœ… Recommended: small model (balanced)")
        else:
            print("âœ… Recommended: base model (fast)")
    else:
        print("âœ… Recommended: base model (CPU mode)")
    
    return cuda_available

if __name__ == "__main__":
    check_gpu() 